[{"content":"Today a colleague of mine who is new to Kubernetes asked me what was the CNI running on our production cluster, I knew the answer but I wanted to doublecheck, just to see if I could climb up to the source of truth.\nMy first reflex was to search in the kube-system namespace for pods with a CNI-like name. And I quickly found what I was looking for :\nWith this I was pretty sure Antrea was the chosen CNI, but I knew there were other ways to verify it.\nWhen a CNI is installed on a cluster you can usually find its binaries somewhere on the node, they are generally located under the /opt/cni directory :\nThe binaries are there, but we can see that we have antrea as well as flannel binaries which are two completely different CNIs, we need to check which of them are used by the kubelet when creating the CNI pods in the cluster.\nWe can do this by checking the /etc/cni/net.d directory that normaly hosts the config file for the cluster\u0026rsquo;s chosen CNI :\nThe problem here is that we have two configuration files, one for flannel and one for antrea which is quite logical taking in account that we previously checked the CNI binaries directory and found both of these CNIs there. Having two configuration files here doesn\u0026rsquo;t help us because we can not determine which one is used and thus which is the CNI used by the cluster.\nThe thing we have to remember here is that the kubelet always execute the first configuration file it finds under the /etc/cni/net.d directory.\nWe thus simply have to check what is the first configuration file that will be found by the kubelet while inspecting the net.d directory :\nAs expected its the antrea config file that will be executed by the kubelet, we are now 100% sure that Antrea is the CNI running in our cluster.\nIt is also possible to check and analyse the logs of the Kubelet in order to find logs related to the CNI with journalctl -u kubelet | less, then search for the pattern or journalctl -u kubelete | grep \u0026lt;pattern\u0026gt;\n","permalink":"https://hdelaunay.fr/posts/which-cni-is-running-in-my-cluster/","summary":"Today a colleague of mine who is new to Kubernetes asked me what was the CNI running on our production cluster, I knew the answer but I wanted to doublecheck, just to see if I could climb up to the source of truth.\nMy first reflex was to search in the kube-system namespace for pods with a CNI-like name. And I quickly found what I was looking for :\nWith this I was pretty sure Antrea was the chosen CNI, but I knew there were other ways to verify it.","title":"Which Cni Is Running in My Cluster ?"},{"content":"Today at my job I managed to get our new product up and running on kubernetes, deployment and statefulsets are working but I wanted to test the app\u0026rsquo;s behavior while accessing it from the outside. As we have multiple services running on the cluster and as we want to secure the connection through TLS it is best to use an Ingress (resources and controllers). I already did the Ingress controller setup earlier, using Ingress-nginx-controller so I just had to create an Ingress-resources and I wanted to discuss about my process here.\nFirst and foremost we need an access to our fully qualified domain certificate. In my case I use a \u0026ldquo;wildcard certificate\u0026rdquo; that backup all the sub domains of my domain :\nIf we take a look at the Ingress TLS section of the kubernetes docs we can find a sample Ingress resource manifest as such :\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: tls-example-ingress spec: tls: - hosts: - https-example.foo.com secretName: testsecret-tls rules: - host: https-example.foo.com http: paths: - path: / pathType: Prefix backend: service: name: service1 port: number: 80 The TLS section of the manifest is mentioning a secret, that is the secret containing the tls certificate and key.\nTo generate the secret :\nkubectl create secret tls \u0026lt;secret-name\u0026gt; --cert=./path-to-your-certificate --key=./path-to-your-key Let\u0026rsquo;s take a look at the secret :\nk get secret -n herve tls-secret -o yaml Output :\napiVersion: v1 data: tls.crt: \u0026lt;encoded-data\u0026gt; tls.key: \u0026lt;encoded-data\u0026gt; kind: Secret metadata: creationTimestamp: \u0026#34;2024-06-21T14:14:22Z\u0026#34; name: tls-secret namespace: herve resourceVersion: \u0026#34;15599806\u0026#34; uid: 8f0h6e84-7678d-67581-ad25-c72acwggbta29 type: kubernetes.io/tls We can see that the kubectl create secret tls command took our cert and key file, encoded them and finally embedded them inside the secret object (I\u0026rsquo;ve hidden my cert and key data here).\nNow we just have to mention the secret under the TLS section of our Ingress resource :\ntls: - hosts: - https-example.foo.com secretName: tls-secret As I am using an Ingress-nginx-controller I have to mention its name under the ingressClassName section :\nKeep in mind that the ingress resource should be created inside the same namespace as the workload it is targeting, it is not done here. And the service name should be the service representing the targeted pods\nThe traffic will now be secured with TLS, and the users will be able to request our service through https.\n","permalink":"https://hdelaunay.fr/posts/adding-tls-to-your-ingress-traffic/","summary":"Today at my job I managed to get our new product up and running on kubernetes, deployment and statefulsets are working but I wanted to test the app\u0026rsquo;s behavior while accessing it from the outside. As we have multiple services running on the cluster and as we want to secure the connection through TLS it is best to use an Ingress (resources and controllers). I already did the Ingress controller setup earlier, using Ingress-nginx-controller so I just had to create an Ingress-resources and I wanted to discuss about my process here.","title":"Adding TLS to Your Ingress Traffic"},{"content":"Pulling images from a private registry with kubernetes In this example I am going to use Harbor.\nFor the sake of best practices and security we first have to create a robot account on harbor :\nThen select the harbor repo on which the robot account will have these permissions :\nYou then have a confirmation message from harbor, keep the secret : we will register it into a kubernetes secret in our cluster.\nNow that we\u0026rsquo;ve created the robot account we can login inside one of our cluster machine (the one you are using kubeadm on) and create a secret containing the robot account secret we\u0026rsquo;ve copied earlier.\nBeware that you should create the secret in the proper namespace !\nkubectl create secret docker-registry harbor-preprod-robot \\ --docker-server=\u0026lt;harbor-registry-fqdn\u0026gt; --docker-username=\u0026lt;robot-username\u0026gt; --docker-password=\u0026lt;robot-secret\u0026gt; -n \u0026lt;your-namespace\u0026gt; --docker-server: you should put the registry fully qualified domain name but do not forget to add the project and repository path to the address. If our project on Harbor is named foo the the address should be : --docker-server=\u0026lt;harbor-registry-fqdn\u0026gt;/foo. This is important because the robot account has rights only on this project and not on the other ones, not mentioning the project inside the docker-server address will result in an error during the image pull.\nThe secret is now active : You can verify its information with this command :\nkubectl get secret harbor-preprod-robot --output=\u0026#34;jsonpath={.data.\\.dockerconfigjson}\u0026#34; | base64 --decode And then decode the base64 encoded text with this one :\necho \u0026#34;c3R...zE2\u0026#34; | base64 --decode Finally, add the imagePullSecrets property and use the secret inside your deployment manifest : Here it is important to mention the tag of the image you want to pull in your deployment. Kubernetes want pull the \u0026ldquo;latest\u0026rdquo; image if you don\u0026rsquo;t specify it, the pull will just crash.\nYour deployment should now be fully ready to be created and in capacity to pull images from your private Harbor registry.\n","permalink":"https://hdelaunay.fr/posts/pull-images-from-your-harbor-registry-with-kubernetes/","summary":"Pulling images from a private registry with kubernetes In this example I am going to use Harbor.\nFor the sake of best practices and security we first have to create a robot account on harbor :\nThen select the harbor repo on which the robot account will have these permissions :\nYou then have a confirmation message from harbor, keep the secret : we will register it into a kubernetes secret in our cluster.","title":"Pull Images From Your Harbor Registry With Kubernetes"},{"content":"I recently had to setup an Ingress in one of the Kubernetes cluster of my job, so I wrote a short explanation for my colleagues that are not aware of Kubernetes Ingresses and since it provide usefull explanations on it I figured I\u0026rsquo;ll also share it here :\nClassic Ingress in Kubernetes An Ingress in Kubernetes is an object that makes microservices available outside the cluster. It routes incoming requests to the appropriate services, much like a traditional load balancer.\nWhen discussing Ingress, it generally involves two main elements:\nIngress Resource: These are the routing rules for incoming traffic. These rules determine how traffic is directed based on the request path, akin to a conventional load balancer. Ingress Controller: This controller exists within the cluster as a pod. It reads the rules from the Ingress resource and directs traffic accordingly. By default, the Ingress controller is only accessible within the cluster. Typically, it is exposed externally using a NodePort service. However, this is not the approach we\u0026rsquo;ve chosen.\nOn-Premises architecture considerations In cloud architectures the Ingress controller is automatically exposed through an external load balancer provisioned automatically by the cloud provider. In an on-premises architecture, we manage the external exposure of the Ingress controller ourselves (and this is the example we will study here).\nIngress-nginx-controller - Traffic Routing We employ an existing Ingress deployment , which exposes an nginx pod acting as reverse proxy within our cluster. Based on the Ingress resources, it routes traffic to the appropriate application pods.\nTo install it :\n# With helm helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace # With kubectl kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml The controller pod and the services should now be running :\nherve@master-node:~$ k get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-controller-c8f499cfc-mmr7j 1/1 Running 0 11d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller LoadBalancer 10.100.77.86 \u0026lt;pending\u0026gt; 80:32729/TCP,443:32366/TCP 11d service/ingress-nginx-controller-admission ClusterIP 10.101.134.15 \u0026lt;none\u0026gt; 443/TCP 11d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 11d NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-c8f499cfc 1 1 1 11d This deployment includes three notable elements:\nnginx-ingress-controller pod: Reads the Ingress resource traffic rules and redirects traffic to ClusterIp services representing the correct pods. nginx-ingress-controller service: This Load Balancer type service exposes our ingress-controller pod outside the cluster. It accepts incoming requests and redirects them to the nginx-controller pod. If the deployment has multiple nginx-controller replicas, the service performs load balancing among these pods. nginx-controller-admission service: Ensures compliance of the Ingress rules before they are communicated to ETCD. Understanding this service is not crucial for basic operations. Thus, our Ingress routes traffic appropriately within our cluster, yet we still lack a node-level load balancing solution.\nHA-Proxy - Load Balancing For load balancing among cluster nodes, two solutions are available to us:\nHA-proxy Metal-lb We opted for HA-proxy. This service will run on a node with external exposure to the cluster, capturing client requests from outside the cluster and redistributing them across the Kubernetes nodes according to its load distribution algorithm, utilizing round-robin (equal distribution across all nodes).\nKeepalived for Resilience To enhance resilience we employ a dual HA-Proxy setup, where both machines are represented by a single virtual IP (VIP) through Keepalived. If the primary HA-Proxy server fails, the secondary takes over seamlessly, and the IP remains consistent throughout. This switch is transparent to the firewall, which consistently routes traffic to the same IP.\nAfter configuring the service, we should see two ip on the network interface of the main haproxy server :\nherve@hapxy1:~$ ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens160: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:50:40:vb:05:x9 brd ff:ff:ff:ff:ff:ff altname enp3s0 inet 192.168.103.11/24 metric 100 brd 192.168.103.255 scope global dynamic ens160 valid_lft 675798sec preferred_lft 675798sec inet 192.168.103.10/24 brd 192.168.103.255 scope global secondary ens160 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:feba:5d0/64 scope link valid_lft forever preferred_lft forever And only one in the backup haproxy network interface :\nnuageherve@hapxy2:~$ ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens160: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:80:33:vb:c0:tu brd ff:ff:ff:ff:ff:ff altname enp3s0 inet 192.168.103.12/24 metric 100 brd 192.168.103.255 scope global dynamic ens160 valid_lft 522201sec preferred_lft 522201sec inet6 fe80::250:56ff:feba:c0cb/64 scope link valid_lft forever preferred_lft forever ","permalink":"https://hdelaunay.fr/posts/routing-traffic-within-your-kubernetes-cluster--ingress-nginx-controller/","summary":"I recently had to setup an Ingress in one of the Kubernetes cluster of my job, so I wrote a short explanation for my colleagues that are not aware of Kubernetes Ingresses and since it provide usefull explanations on it I figured I\u0026rsquo;ll also share it here :\nClassic Ingress in Kubernetes An Ingress in Kubernetes is an object that makes microservices available outside the cluster. It routes incoming requests to the appropriate services, much like a traditional load balancer.","title":"Routing Traffic Within Your Kubernetes Cluster : Ingress-Nginx Controller"},{"content":"When you try to optimize the size of an image it is possible to display the size of every layers :\ndocker history \u0026lt;image-id\u0026gt; For example :\n(ins)❯ docker history 7383c266ef25 IMAGE CREATED CREATED BY SIZE COMMENT 7383c266ef25 12 days ago CMD [\u0026#34;nginx\u0026#34; \u0026#34;-g\u0026#34; \u0026#34;daemon off;\u0026#34;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago STOPSIGNAL SIGQUIT 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago EXPOSE map[80/tcp:{}] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY 30-tune-worker-processes.sh /docker-ent… 4.62kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY 20-envsubst-on-templates.sh /docker-ent… 3.02kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY 15-local-resolvers.envsh /docker-entryp… 336B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY 10-listen-on-ipv6-by-default.sh /docker… 2.12kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY docker-entrypoint.sh / # buildkit 1.62kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago RUN /bin/sh -c set -x \u0026amp;\u0026amp; groupadd --syst… 113MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENV PKG_RELEASE=1~bookworm 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENV NJS_RELEASE=2~bookworm 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENV NJS_VERSION=0.8.4 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENV NGINX_VERSION=1.25.5 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago LABEL maintainer=NGINX Docker Maintainers \u0026lt;d… 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago /bin/sh -c #(nop) CMD [\u0026#34;bash\u0026#34;] 0B \u0026lt;missing\u0026gt; 12 days ago /bin/sh -c #(nop) ADD file:4b1be1de1a1e5aa60… 74.8MB ","permalink":"https://hdelaunay.fr/posts/optimizing-docker-images-size/","summary":"When you try to optimize the size of an image it is possible to display the size of every layers :\ndocker history \u0026lt;image-id\u0026gt; For example :\n(ins)❯ docker history 7383c266ef25 IMAGE CREATED CREATED BY SIZE COMMENT 7383c266ef25 12 days ago CMD [\u0026#34;nginx\u0026#34; \u0026#34;-g\u0026#34; \u0026#34;daemon off;\u0026#34;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago STOPSIGNAL SIGQUIT 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago EXPOSE map[80/tcp:{}] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] 0B buildkit.","title":"Optimizing Docker Images Size"},{"content":"I\u0026rsquo;m a french DevOps engineer that looks for perfecting his craft.\n","permalink":"https://hdelaunay.fr/aboutme/","summary":"I\u0026rsquo;m a french DevOps engineer that looks for perfecting his craft.","title":""}]