[{"content":"It has been some time that I\u0026rsquo;ve been searching for a way to expose my self-hosted apps without opening my network to the internet. I\u0026rsquo;ve finally found the solution : Cloudflare tunnels. They only require an outbound connection and remove the need to expose any ports directly.\nRequirements:\nA Cloudflare-managed domain (you can also transfer an existing domain to Cloudflare) A Kubernetes cluster (in my case, managed with FluxCD following GitOps best practices) Authenticating and creating the tunnel First, download cloudflared on your machine\nThen authenticate to Cloudflare :\ncloudflared tunnel login\nThis will open your browser, ask for your Cloudflare credentials, and create a cert at ~/.cloudflared/cert.pem\nNext, create the tunnel from the CLI. In my case, I want to expose the Linkding app:\ncloudflared tunnel create linkding\nThis command generates a .json file containing the tunnel\u0026rsquo;s tokens. This is the file that allows us to create a tunnel from our cloudflared deployment to the cloudflare network.\nWe\u0026rsquo;ll want to transform this file in a kubernetes secret inside of our cluster :\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=\u0026lt;YOUR_TUNNEL_ID\u0026gt;.json \\ --dry-run=client \\ -o yaml \u0026gt; tunnel_credentials.yaml Encrypting secrets with SOPS and Age As we are using FluxCD as our continuous deployment tool, we need to keep in mind that all our code is going to get pushed to Github. We need a way to encrypt our secret before pushing it to our repo.\nFor this we\u0026rsquo;ll use sops encryption with an age key as described in Flux\u0026rsquo;s docs\nEssentially, we\u0026rsquo;ll encrypt our tunnel-credentials secret with our age public key. We\u0026rsquo;ll then want to place our age private key inside of our cluster, and we\u0026rsquo;ll do it manually. The tunnel-credentials secret will then be decripted by Flux, using the sops private key living inside of our cluster as a secret object.\nLet\u0026rsquo;s first create the age keypair :\n# if not already installed $ brew install sops age $ age-keygen -o age.agekey Public key: age1helqcqsh9464r8chnwc2fzj8uv7vr5ntnsft0tn45v2xtz0hpfwq98cmsg We then imperatively create a secret object on the cluster, containing our age private key\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin the key name must end with .agekey to be detected as an age key\nNow that the private key is in the cluster, we can encrypt the tunnel-credentials file with sops and our age public key\ncat age.agekey sops --age=age1helqcqsh9464r8chnwc2fzj8uv7vr5ntnsft0tn45v2xtz0hpfwq98cmsg \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place tunnel-credentials.yaml Configuring FluxCD for Decryption In order for Flux to know it has to decrypt tunnel-credentials with Sops, you\u0026rsquo;ll need to add this in the Flux Kustomization object pointing to your apps :\n# note that the secret is called sops-age in my cluster # it\u0026#39;s our age private key decryption: provider: sops secretRef: name: sops-age In my case, I want Flux to be aware that there will be a Sops encrypted file in my /apps/staging directory :\napiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m0s retryInterval: 1m timeout: 5m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging prune: true decryption: provider: sops secretRef: name: sops-age At this point, we can commit and push our code safely, Flux will pick it up and apply it in our cluster, decoding tunnel-credentials.yaml with the age secret.\nDeploying Cloudflared in the Cluster Right now Flux can manage the tunnel-credentials Secret securely. But nothing is actually running the tunnel yet. We need a cloudflared Deployment in the cluster that connects to Cloudflare’s edge and routes traffic to our internal app.\nHere’s an example Deployment and ConfigMap that run cloudflared with the tunnel we created earlier (in my case, linkding):\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel # Points cloudflared to the config file, which configures what # cloudflared will actually do. This file is created by a ConfigMap # below. - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true # Each tunnel has an associated \u0026#34;credentials file\u0026#34; which authorizes machines # to run the tunnel. cloudflared will read this file from its local filesystem, # and it\u0026#39;ll be stored in a k8s secret. - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials # Create a config.yaml file from the ConfigMap below. - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: \\| # Name of the tunnel you want to run tunnel: linkding credentials-file: /etc/cloudflared/creds/credentials.json metrics: 0.0.0.0:2000 no-autoupdate: true ingress: - hostname: linkding.hervedelaunay.com service: http://linkding:9090 - hostname: hello.example.com service: hello_world - service: http_status:404 This tells cloudflared to:\nuse the credentials we stored in the tunnel-credentials Secret, run the tunnel named linkding, and route requests from linkding.hervedelaunay.com to the Linkding service running in the cluster. Wrapping Up With this setup, you can safely manage Cloudflare Tunnel credentials in Git, thanks to FluxCD and SOPS.\nYour services stay private - only outbound traffic is needed - while still being accessible securely through Cloudflare’s network.\nThis approach works not just for Linkding, but for any self-hosted app you want to expose securely.\n","permalink":"http://localhost:1313/posts/secure-gitops--exposing-internal-kubernetes-services-with-cloudflare-tunnels/","summary":"\u003cp\u003eIt has been some time that I\u0026rsquo;ve been searching for a way to expose my self-hosted apps without opening my network to the internet. I\u0026rsquo;ve finally found the solution : Cloudflare tunnels. They only require an outbound connection and remove the need to expose any ports directly.\u003c/p\u003e\n\u003cp\u003eRequirements:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA Cloudflare-managed domain (you can also transfer an existing domain to Cloudflare)\u003c/li\u003e\n\u003cli\u003eA Kubernetes cluster (in my case, managed with FluxCD following GitOps best practices)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"authenticating-and-creating-the-tunnel\"\u003eAuthenticating and creating the tunnel\u003c/h2\u003e\n\u003cp\u003eFirst, \u003ca href=\"https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/downloads/\"\u003edownload cloudflared\u003c/a\u003e on your machine\u003c/p\u003e","title":"Secure GitOps : Exposing Internal Kubernetes Services With Cloudflare Tunnels"},{"content":"In the world of Kubernetes and Helm, managing secrets securely and efficiently is crucial, especially when dealing with production databases. This article chronicles my journey to implement a dynamic secret generation mechanism for a StatefulSet\u0026rsquo;s database container using Helm.\nMy objective was to generate a password for the database container of a StatefulSet using Helm. The requirements were to create the secret during the initial installation, to reuse the same secret when upgrading the release and to ensure the secret persists even if the release is deleted\nLet\u0026rsquo;s walk through the evolution of my solution, examining each attempt and its flaws before arriving at the final (and optimal ?) approach.\nThe first attempt My initial attempt looked like this:\napiVersion: v1 kind: Secret metadata: name: {{ .Values.secrets.myapp.secretName }} annotations: \u0026#34;helm.sh/resource-policy\u0026#34;: \u0026#34;keep\u0026#34; type: Opaque data: {{- $secretObj := (lookup \u0026#34;v1\u0026#34; \u0026#34;Secret\u0026#34; .Release.Namespace .Values.secrets.myapp.secretName) | default dict }} {{- $secretData := (get $secretObj \u0026#34;data\u0026#34;) | default dict }} {{- $mssqlPasswordKey := .Values.secrets.myapp.mssqlPasswordKey }} {{- $dbSecret := (get $secretData $mssqlPasswordKey) | default (randAlphaNum 16) }} {{ $mssqlPasswordKey }}: {{ $dbSecret | b64enc | quote }} What went wrong This implementation had a critical flaw: the randAlphaNum 16 function was called on every Helm operation, not just during the initial installation. This led to:\nA new random string being generated and appended to the existing password on each upgrade. The secret becoming longer and wrongly encoded due to repeated base64 encoding. The second attempt: conditional logic I then tried a more sophisticated approach:\napiVersion: v1 kind: Secret metadata: name: {{ .Values.secrets.myapp.secretName }} annotations: \u0026#34;helm.sh/resource-policy\u0026#34;: \u0026#34;keep\u0026#34; type: Opaque data: {{- $secretObj := (lookup \u0026#34;v1\u0026#34; \u0026#34;Secret\u0026#34; .Release.Namespace .Values.secrets.myapp.secretName) | default dict }} {{- $secretData := (get $secretObj \u0026#34;data\u0026#34;) | default dict }} {{- $mssqlPasswordKey := .Values.secrets.myapp.mssqlPasswordKey }} {{- if not (hasKey $secretData $mssqlPasswordKey) }} {{- $dbSecret := (randAlphaNum 16) }} {{ $mssqlPasswordKey }}: {{ $dbSecret | b64enc | quote }} {{- else }} {{- $dbSecret := get $secretData $mssqlPasswordKey }} {{ $mssqlPasswordKey }}: {{ $dbSecret | b64enc | quote }} {{- end}} What went wrong While this version was closer to my goal, it still had several issues:\nIn the else branch, I was retrieving the base64-encoded value from the existing secret. I then applied b64enc again, resulting in double encoding. The values in $secretData were already strings, so get $secretData $mssqlPasswordKey returned a base64-encoded string, not the raw value. The third attempt: simplified but flawed My third attempt aimed for simplicity:\n{{- $secret := (lookup \u0026#34;v1\u0026#34; \u0026#34;Secret\u0026#34; .Release.Namespace .Values.secrets.myapp.secretName) -}} apiVersion: v1 kind: Secret metadata: name: {{ .Values.secrets.myapp.secretName }} annotations: \u0026#34;helm.sh/resource-policy\u0026#34;: \u0026#34;keep\u0026#34; type: Opaque data: {{ $mssqlPasswordKey := .Values.secrets.myapp.mssqlPasswordKey -}} {{ if $secret -}} {{ $mssqlPasswordKey }}: {{ get $secret $mssqlPasswordKey }} {{ else -}} {{ $mssqlPasswordKey }}: {{ randAlphaNum 16 | b64enc | quote }} {{ end -}} What went wrong This approach had a fundamental misunderstanding of how secret data is stored:\nI tried to access secret data directly from the $secret object, which doesn\u0026rsquo;t work as expected. The correct way would have been get $secret.data $mssqlPasswordKey, but this would still return a base64-encoded value. I didn\u0026rsquo;t apply b64enc | quote to the existing value, potentially causing formatting issues. The final solution After my journey of trial and error, I arrived at a simple but yet effective solution:\n{{- if .Values.secrets.mpleo.create -}} apiVersion: v1 kind: Secret metadata: name: {{ .Values.secrets.mpleo.secretName }} annotations: \u0026#34;helm.sh/resource-policy\u0026#34;: \u0026#34;keep\u0026#34; type: Opaque data: {{ .Values.secrets.mpleo.mssqlPasswordKey }}: {{ randAlphaNum 16 | b64enc | quote }} {{- end -}} Why this works This solution is elegant and effective for several reasons:\nControlled Creation: The if .Values.secrets.mpleo.create condition allows us to control when the secret is created, typically only during the initial installation. Persistence: The helm.sh/resource-policy: keep annotation ensures that the secret isn\u0026rsquo;t deleted when the release is uninstalled, preserving it for future upgrades. One-time Password Generation: randAlphaNum 16 is only called when the secret is first created, ensuring the password remains consistent across upgrades. Proper Encoding: The generated password is correctly base64 encoded and quoted, ready for use as a Kubernetes secret. Simplicity: By avoiding complex logic for checking existing secrets, we reduce the chances of errors and make the template easier to maintain. Good ol\u0026rsquo; KISS Well, this exercise teached me a lesson : I tend to over-engineer things. Not later than today a friend of mine told me about Elon Musk\u0026rsquo;s engineering framework in which he states that one should try to think in a \u0026ldquo;deletion\u0026rdquo; mindset when designin features of a product. I think I should embrace that way of thinking in my enineering, trying to delete inessential logic and code when I can.\n","permalink":"http://localhost:1313/posts/mastering-helm--a-journey-to-dynamic-secret-generation/","summary":"\u003cp\u003eIn the world of Kubernetes and Helm, managing secrets securely and efficiently is crucial, especially when dealing with production databases. This article chronicles my journey to implement a dynamic secret generation mechanism for a StatefulSet\u0026rsquo;s database container using Helm.\u003c/p\u003e\n\u003cp\u003eMy objective was to generate a password for the database container of a StatefulSet using Helm. The requirements were to create the secret during the initial installation, to reuse the same secret when upgrading the release and to ensure the secret persists even if the release is deleted\u003c/p\u003e","title":"Mastering Helm : A Journey to Dynamic Secret Generation"},{"content":"Today a colleague of mine who is new to Kubernetes asked me what was the CNI running on our production cluster, I knew the answer but I wanted to doublecheck, just to see if I could climb up to the source of truth.\nMy first reflex was to search in the kube-system namespace for pods with a CNI-like name. And I quickly found what I was looking for :\nWith this I was pretty sure Antrea was the chosen CNI, but I knew there were other ways to verify it.\nWhen a CNI is installed on a cluster you can usually find its binaries somewhere on the node, they are generally located under the /opt/cni directory :\nThe binaries are there, but we can see that we have antrea as well as flannel binaries which are two completely different CNIs, we need to check which of them are used by the kubelet when creating the CNI pods in the cluster.\nWe can do this by checking the /etc/cni/net.d directory that normaly hosts the config file for the cluster\u0026rsquo;s chosen CNI :\nThe problem here is that we have two configuration files, one for flannel and one for antrea which is quite logical taking in account that we previously checked the CNI binaries directory and found both of these CNIs there. Having two configuration files here doesn\u0026rsquo;t help us because we can not determine which one is used and thus which is the CNI used by the cluster.\nThe thing we have to remember here is that the kubelet always execute the first configuration file it finds under the /etc/cni/net.d directory.\nWe thus simply have to check what is the first configuration file that will be found by the kubelet while inspecting the net.d directory :\nAs expected its the antrea config file that will be executed by the kubelet, we are now 100% sure that Antrea is the CNI running in our cluster.\nIt is also possible to check and analyse the logs of the Kubelet in order to find logs related to the CNI with journalctl -u kubelet | less, then search for the pattern or journalctl -u kubelete | grep \u0026lt;pattern\u0026gt;\n","permalink":"http://localhost:1313/posts/which-cni-is-running-in-my-cluster/","summary":"\u003cp\u003eToday a colleague of mine who is new to Kubernetes asked me what was the CNI running on our production cluster, I knew the answer but I wanted to doublecheck, just to see if I could climb up to the source of truth.\u003c/p\u003e\n\u003cp\u003eMy first reflex was to search in the kube-system namespace for pods with a CNI-like name. And I quickly found what I was looking for :\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"cni-1\" loading=\"lazy\" src=\"/which-cni-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWith this I was pretty sure \u003cstrong\u003eAntrea\u003c/strong\u003e was the chosen CNI, but I knew there were other ways to verify it.\u003c/p\u003e","title":"Which Cni Is Running in My Cluster ?"},{"content":"Today at my job I managed to get our new product up and running on kubernetes, deployment and statefulsets are working but I wanted to test the app\u0026rsquo;s behavior while accessing it from the outside. As we have multiple services running on the cluster and as we want to secure the connection through TLS it is best to use an Ingress (resources and controllers). I already did the Ingress controller setup earlier, using Ingress-nginx-controller so I just had to create an Ingress-resources and I wanted to discuss about my process here.\nFirst and foremost we need an access to our fully qualified domain certificate. In my case I use a \u0026ldquo;wildcard certificate\u0026rdquo; that backup all the sub domains of my domain :\nIf we take a look at the Ingress TLS section of the kubernetes docs we can find a sample Ingress resource manifest as such :\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: tls-example-ingress spec: tls: - hosts: - https-example.foo.com secretName: testsecret-tls rules: - host: https-example.foo.com http: paths: - path: / pathType: Prefix backend: service: name: service1 port: number: 80 The TLS section of the manifest is mentioning a secret, that is the secret containing the tls certificate and key.\nTo generate the secret :\nkubectl create secret tls \u0026lt;secret-name\u0026gt; --cert=./path-to-your-certificate --key=./path-to-your-key Let\u0026rsquo;s take a look at the secret :\nk get secret -n herve tls-secret -o yaml Output :\napiVersion: v1 data: tls.crt: \u0026lt;encoded-data\u0026gt; tls.key: \u0026lt;encoded-data\u0026gt; kind: Secret metadata: creationTimestamp: \u0026#34;2024-06-21T14:14:22Z\u0026#34; name: tls-secret namespace: herve resourceVersion: \u0026#34;15599806\u0026#34; uid: 8f0h6e84-7678d-67581-ad25-c72acwggbta29 type: kubernetes.io/tls We can see that the kubectl create secret tls command took our cert and key file, encoded them and finally embedded them inside the secret object (I\u0026rsquo;ve hidden my cert and key data here).\nNow we just have to mention the secret under the TLS section of our Ingress resource :\ntls: - hosts: - https-example.foo.com secretName: tls-secret As I am using an Ingress-nginx-controller I have to mention its name under the ingressClassName section :\nKeep in mind that the ingress resource should be created inside the same namespace as the workload it is targeting, it is not done here. And the service name should be the service representing the targeted pods\nThe traffic will now be secured with TLS, and the users will be able to request our service through https.\n","permalink":"http://localhost:1313/posts/adding-tls-to-your-ingress-traffic/","summary":"\u003cp\u003eToday at my job I managed to get our new product up and running on kubernetes, deployment and statefulsets are working but I wanted to test the app\u0026rsquo;s behavior while accessing it from the outside. As we have multiple services running on the cluster and as we want to secure the connection through TLS it is best to use an Ingress (resources and controllers). I already did the Ingress controller setup earlier, using \u003ca href=\"https://kubernetes.github.io/ingress-nginx/\"\u003eIngress-nginx-controller\u003c/a\u003e so I just had to create an Ingress-resources and I wanted to discuss about my process here.\u003c/p\u003e","title":"Adding TLS to Your Ingress Traffic"},{"content":"Pulling images from a private registry with kubernetes In this example I am going to use Harbor.\nFor the sake of best practices and security we first have to create a robot account on harbor :\nThen select the harbor repo on which the robot account will have these permissions :\nYou then have a confirmation message from harbor, keep the secret : we will register it into a kubernetes secret in our cluster.\nNow that we\u0026rsquo;ve created the robot account we can login inside one of our cluster machine (the one you are using kubeadm on) and create a secret containing the robot account secret we\u0026rsquo;ve copied earlier.\nBeware that you should create the secret in the proper namespace !\nkubectl create secret docker-registry harbor-preprod-robot \\ --docker-server=\u0026lt;harbor-registry-fqdn\u0026gt; --docker-username=\u0026lt;robot-username\u0026gt; --docker-password=\u0026lt;robot-secret\u0026gt; -n \u0026lt;your-namespace\u0026gt; --docker-server: you should put the registry fully qualified domain name but do not forget to add the project and repository path to the address. If our project on Harbor is named foo the the address should be : --docker-server=\u0026lt;harbor-registry-fqdn\u0026gt;/foo. This is important because the robot account has rights only on this project and not on the other ones, not mentioning the project inside the docker-server address will result in an error during the image pull.\nThe secret is now active : You can verify its information with this command :\nkubectl get secret harbor-preprod-robot --output=\u0026#34;jsonpath={.data.\\.dockerconfigjson}\u0026#34; | base64 --decode And then decode the base64 encoded text with this one :\necho \u0026#34;c3R...zE2\u0026#34; | base64 --decode Finally, add the imagePullSecrets property and use the secret inside your deployment manifest : Here it is important to mention the tag of the image you want to pull in your deployment. Kubernetes want pull the \u0026ldquo;latest\u0026rdquo; image if you don\u0026rsquo;t specify it, the pull will just crash.\nYour deployment should now be fully ready to be created and in capacity to pull images from your private Harbor registry.\n","permalink":"http://localhost:1313/posts/pull-images-from-your-harbor-registry-with-kubernetes/","summary":"\u003ch2 id=\"pulling-images-from-a-private-registry-with-kubernetes\"\u003ePulling images from a private registry with kubernetes\u003c/h2\u003e\n\u003cp\u003eIn this example I am going to use Harbor.\u003c/p\u003e\n\u003cp\u003eFor the sake of best practices and security we first have to create a robot account on harbor :\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"harbor-1\" loading=\"lazy\" src=\"/harbor-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"harbor-2\" loading=\"lazy\" src=\"/harbor-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"harbor-3\" loading=\"lazy\" src=\"/harbor-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThen select the harbor repo on which the robot account will have these permissions :\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"harbor-4\" loading=\"lazy\" src=\"/harbor-4.png\"\u003e\u003c/p\u003e\n\u003cp\u003eYou then have a confirmation message from harbor, keep the secret : we will register it into a kubernetes secret in our cluster.\u003c/p\u003e","title":"Pull Images From Your Harbor Registry With Kubernetes"},{"content":"I recently had to setup an Ingress in one of the Kubernetes cluster of my job, so I wrote a short explanation for my colleagues that are not aware of Kubernetes Ingresses and since it provide usefull explanations on it I figured I\u0026rsquo;ll also share it here :\nClassic Ingress in Kubernetes An Ingress in Kubernetes is an object that makes microservices available outside the cluster. It routes incoming requests to the appropriate services, much like a traditional load balancer.\nWhen discussing Ingress, it generally involves two main elements:\nIngress Resource: These are the routing rules for incoming traffic. These rules determine how traffic is directed based on the request path, akin to a conventional load balancer. Ingress Controller: This controller exists within the cluster as a pod. It reads the rules from the Ingress resource and directs traffic accordingly. By default, the Ingress controller is only accessible within the cluster. Typically, it is exposed externally using a NodePort service. However, this is not the approach we\u0026rsquo;ve chosen.\nOn-Premises architecture considerations In cloud architectures the Ingress controller is automatically exposed through an external load balancer provisioned automatically by the cloud provider. In an on-premises architecture, we manage the external exposure of the Ingress controller ourselves (and this is the example we will study here).\nIngress-nginx-controller - Traffic Routing We employ an existing Ingress deployment , which exposes an nginx pod acting as reverse proxy within our cluster. Based on the Ingress resources, it routes traffic to the appropriate application pods.\nTo install it :\n# With helm helm upgrade --install ingress-nginx ingress-nginx \\ --repo https://kubernetes.github.io/ingress-nginx \\ --namespace ingress-nginx --create-namespace # With kubectl kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml The controller pod and the services should now be running :\nherve@master-node:~$ k get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-controller-c8f499cfc-mmr7j 1/1 Running 0 11d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller LoadBalancer 10.100.77.86 \u0026lt;pending\u0026gt; 80:32729/TCP,443:32366/TCP 11d service/ingress-nginx-controller-admission ClusterIP 10.101.134.15 \u0026lt;none\u0026gt; 443/TCP 11d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 11d NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-c8f499cfc 1 1 1 11d This deployment includes three notable elements:\nnginx-ingress-controller pod: Reads the Ingress resource traffic rules and redirects traffic to ClusterIp services representing the correct pods. nginx-ingress-controller service: This Load Balancer type service exposes our ingress-controller pod outside the cluster. It accepts incoming requests and redirects them to the nginx-controller pod. If the deployment has multiple nginx-controller replicas, the service performs load balancing among these pods. nginx-controller-admission service: Ensures compliance of the Ingress rules before they are communicated to ETCD. Understanding this service is not crucial for basic operations. Thus, our Ingress routes traffic appropriately within our cluster, yet we still lack a node-level load balancing solution.\nHA-Proxy - Load Balancing For load balancing among cluster nodes, two solutions are available to us:\nHA-proxy Metal-lb We opted for HA-proxy. This service will run on a node with external exposure to the cluster, capturing client requests from outside the cluster and redistributing them across the Kubernetes nodes according to its load distribution algorithm, utilizing round-robin (equal distribution across all nodes).\nKeepalived for Resilience To enhance resilience we employ a dual HA-Proxy setup, where both machines are represented by a single virtual IP (VIP) through Keepalived. If the primary HA-Proxy server fails, the secondary takes over seamlessly, and the IP remains consistent throughout. This switch is transparent to the firewall, which consistently routes traffic to the same IP.\nAfter configuring the service, we should see two ip on the network interface of the main haproxy server :\nherve@hapxy1:~$ ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens160: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:50:40:vb:05:x9 brd ff:ff:ff:ff:ff:ff altname enp3s0 inet 192.168.103.11/24 metric 100 brd 192.168.103.255 scope global dynamic ens160 valid_lft 675798sec preferred_lft 675798sec inet 192.168.103.10/24 brd 192.168.103.255 scope global secondary ens160 valid_lft forever preferred_lft forever inet6 fe80::250:56ff:feba:5d0/64 scope link valid_lft forever preferred_lft forever And only one in the backup haproxy network interface :\nnuageherve@hapxy2:~$ ip addr show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens160: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:80:33:vb:c0:tu brd ff:ff:ff:ff:ff:ff altname enp3s0 inet 192.168.103.12/24 metric 100 brd 192.168.103.255 scope global dynamic ens160 valid_lft 522201sec preferred_lft 522201sec inet6 fe80::250:56ff:feba:c0cb/64 scope link valid_lft forever preferred_lft forever ","permalink":"http://localhost:1313/posts/routing-traffic-within-your-kubernetes-cluster--ingress-nginx-controller/","summary":"\u003cp\u003eI recently had to setup an Ingress in one of the Kubernetes cluster of my job, so I wrote a short explanation for my colleagues that are not aware of Kubernetes Ingresses and since it provide usefull explanations on it I figured I\u0026rsquo;ll also share it here :\u003c/p\u003e\n\u003ch2 id=\"classic-ingress-in-kubernetes\"\u003eClassic Ingress in Kubernetes\u003c/h2\u003e\n\u003cp\u003eAn Ingress in Kubernetes is an object that makes microservices available outside the cluster. It routes incoming requests to the appropriate services, much like a traditional load balancer.\u003c/p\u003e","title":"Routing Traffic Within Your Kubernetes Cluster : Ingress-Nginx Controller"},{"content":"When you try to optimize the size of an image it is possible to display the size of every layers :\ndocker history \u0026lt;image-id\u0026gt; For example :\n(ins)❯ docker history 7383c266ef25 IMAGE CREATED CREATED BY SIZE COMMENT 7383c266ef25 12 days ago CMD [\u0026#34;nginx\u0026#34; \u0026#34;-g\u0026#34; \u0026#34;daemon off;\u0026#34;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago STOPSIGNAL SIGQUIT 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago EXPOSE map[80/tcp:{}] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;] 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY 30-tune-worker-processes.sh /docker-ent… 4.62kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY 20-envsubst-on-templates.sh /docker-ent… 3.02kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY 15-local-resolvers.envsh /docker-entryp… 336B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY 10-listen-on-ipv6-by-default.sh /docker… 2.12kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago COPY docker-entrypoint.sh / # buildkit 1.62kB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago RUN /bin/sh -c set -x \u0026amp;\u0026amp; groupadd --syst… 113MB buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENV PKG_RELEASE=1~bookworm 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENV NJS_RELEASE=2~bookworm 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENV NJS_VERSION=0.8.4 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago ENV NGINX_VERSION=1.25.5 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago LABEL maintainer=NGINX Docker Maintainers \u0026lt;d… 0B buildkit.dockerfile.v0 \u0026lt;missing\u0026gt; 12 days ago /bin/sh -c #(nop) CMD [\u0026#34;bash\u0026#34;] 0B \u0026lt;missing\u0026gt; 12 days ago /bin/sh -c #(nop) ADD file:4b1be1de1a1e5aa60… 74.8MB ","permalink":"http://localhost:1313/posts/optimizing-docker-images-size/","summary":"\u003cp\u003eWhen you try to optimize the size of an image it is possible to display the size of every layers :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker history \u0026lt;image-id\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFor example :\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e(\u003c/span\u003eins\u003cspan style=\"color:#f92672\"\u003e)\u003c/span\u003e❯ docker history 7383c266ef25\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eIMAGE          CREATED       CREATED BY                                      SIZE      COMMENT\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e7383c266ef25   \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   CMD \u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nginx\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;-g\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;daemon off;\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e                0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   STOPSIGNAL SIGQUIT                              0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   EXPOSE map\u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e80/tcp:\u003cspan style=\"color:#f92672\"\u003e{}]\u003c/span\u003e                           0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   ENTRYPOINT \u003cspan style=\"color:#f92672\"\u003e[\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;/docker-entrypoint.sh\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e]\u003c/span\u003e            0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   COPY 30-tune-worker-processes.sh /docker-ent…   4.62kB    buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   COPY 20-envsubst-on-templates.sh /docker-ent…   3.02kB    buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   COPY 15-local-resolvers.envsh /docker-entryp…   336B      buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   COPY 10-listen-on-ipv6-by-default.sh /docker…   2.12kB    buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   COPY docker-entrypoint.sh / \u003cspan style=\"color:#75715e\"\u003e# buildkit          1.62kB    buildkit.dockerfile.v0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   RUN /bin/sh -c set -x     \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e groupadd --syst…   113MB     buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   ENV PKG_RELEASE\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e1~bookworm                      0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   ENV NJS_RELEASE\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e2~bookworm                      0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   ENV NJS_VERSION\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e0.8.4                           0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   ENV NGINX_VERSION\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e1.25.5                        0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   LABEL maintainer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eNGINX Docker Maintainers \u0026lt;d…   0B        buildkit.dockerfile.v0\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   /bin/sh -c \u003cspan style=\"color:#75715e\"\u003e#(nop)  CMD [\u0026#34;bash\u0026#34;]                 0B\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u0026lt;missing\u0026gt;      \u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e days ago   /bin/sh -c \u003cspan style=\"color:#75715e\"\u003e#(nop) ADD file:4b1be1de1a1e5aa60…   74.8MB\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Optimizing Docker Images Size"},{"content":"I\u0026rsquo;m a french DevOps engineer that looks for perfecting his craft.\n","permalink":"http://localhost:1313/aboutme/","summary":"\u003cp\u003eI\u0026rsquo;m a french DevOps engineer that looks for perfecting his craft.\u003c/p\u003e","title":""}]